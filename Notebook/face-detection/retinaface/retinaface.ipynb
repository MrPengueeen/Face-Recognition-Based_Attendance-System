{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d615562",
   "metadata": {},
   "outputs": [],
   "source": [
    "from retinaface import RetinaFace\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "resp = RetinaFace.detect_faces(\"./Test-Images/2.jpg\")\n",
    "img = cv2.imread(\"./Test-Images/1.jpg\")\n",
    "for key in resp.keys():\n",
    "    face = resp[key]\n",
    "    xmin = face['facial_area'][0]\n",
    "    ymin = face['facial_area'][1]\n",
    "    xmax = face['facial_area'][2]\n",
    "    ymax = face['facial_area'][3]\n",
    "    cv2.rectangle(img, (xmin, ymin), (xmax, ymax), color=(255,0,0), thickness=2)\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n",
    "\n",
    "cv2.namedWindow(\"Detected Faces\", cv2.WINDOW_NORMAL) \n",
    "  \n",
    "cv2.resizeWindow(\"Detected Faces\", 1000, 700) \n",
    "\n",
    "cv2.imshow(\"Detected Faces\", img)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a721f8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepface import DeepFace\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "faces = DeepFace.extract_faces(img_path = \"./Test-Images/5.jpg\", \n",
    "        target_size = (200, 200), \n",
    "        detector_backend = 'fastmtcnn'\n",
    ")\n",
    "# print(faces[0])\n",
    "for face in faces:\n",
    "    plt.imshow(face['face'])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a557cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Temporary Database for the classroom\n",
    "\n",
    "from deepface import DeepFace\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import time\n",
    "onlyfiles = [f for f in listdir('./Verification-Images') if isfile(join('./Verification-Images', f))]\n",
    "\n",
    "database = []\n",
    "\n",
    "start = time.time()\n",
    "for file in onlyfiles:\n",
    "    name = file.rstrip('.jpg')\n",
    "    embedding = DeepFace.represent('./Verification-Images/{file}'.format(file=file), model_name = 'Facenet', align=True, detector_backend='retinaface', enforce_detection = False)\n",
    "    temp = {\n",
    "        'name': name,\n",
    "        'face_embedding': embedding,\n",
    "    }\n",
    "    database.append(temp)\n",
    "end = time.time()\n",
    "database_creation_time = end - start\n",
    "print('Time taken for creating face embedding database: {time} seconds'.format(time=database_creation_time))\n",
    "print('Number of student information in the database: {number}'.format(number=len(database)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8618bb64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "# Create temporary database from JSON for the classroom\n",
    "\n",
    "import json\n",
    "from deepface import DeepFace\n",
    "from retinaface import RetinaFace\n",
    "\n",
    "import time\n",
    "\n",
    "f = open('./session21-22/21-22.json')\n",
    "database = json.load(f)\n",
    "f.close()\n",
    "i = 1\n",
    "for student in database:\n",
    "    image = student['image']\n",
    "    embedding = DeepFace.represent(image, model_name = 'Facenet', align=True, detector_backend='retinaface', enforce_detection = False)\n",
    "    student['face_embedding'] = embedding\n",
    "    print(i)\n",
    "    i+=1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fe7b24d0-bcf2-491c-9a1d-36f7b8a0cf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-keras\n",
      "  Downloading tf_keras-2.16.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting tensorflow<2.17,>=2.16 (from tf-keras)\n",
      "  Using cached tensorflow-2.16.2-cp310-cp310-win_amd64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.2 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.0.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (23.5.26)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.10.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (16.0.6)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (4.23.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (1.60.0)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.4.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.31.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (1.26.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.42.0)\n",
      "Requirement already satisfied: rich in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (13.7.0)\n",
      "Requirement already satisfied: namex in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.0.7)\n",
      "Requirement already satisfied: optree in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2023.11.17)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.5.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.1.3)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (2.17.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\aonmo\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.2->tensorflow<2.17,>=2.16->tf-keras) (0.1.2)\n",
      "Downloading tf_keras-2.16.0-py3-none-any.whl (1.7 MB)\n",
      "   ---------------------------------------- 0.0/1.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.0/1.7 MB 1.4 MB/s eta 0:00:02\n",
      "    --------------------------------------- 0.0/1.7 MB 393.8 kB/s eta 0:00:05\n",
      "   -- ------------------------------------- 0.1/1.7 MB 939.4 kB/s eta 0:00:02\n",
      "   --- ------------------------------------ 0.1/1.7 MB 774.0 kB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 0.2/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 0.3/1.7 MB 927.4 kB/s eta 0:00:02\n",
      "   ------- -------------------------------- 0.3/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "   -------- ------------------------------- 0.4/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------- ----------------------------- 0.5/1.7 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------- -------------------------- 0.6/1.7 MB 1.2 MB/s eta 0:00:01\n",
      "   --------------- ------------------------ 0.7/1.7 MB 1.3 MB/s eta 0:00:01\n",
      "   ----------------- ---------------------- 0.7/1.7 MB 1.4 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 0.8/1.7 MB 1.4 MB/s eta 0:00:01\n",
      "   --------------------- ------------------ 0.9/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 1.0/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.1/1.7 MB 1.5 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.2/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 1.3/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 1.4/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.5/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.6/1.7 MB 1.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 1.7/1.7 MB 1.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.7/1.7 MB 1.6 MB/s eta 0:00:00\n",
      "Using cached tensorflow-2.16.2-cp310-cp310-win_amd64.whl (2.1 kB)\n",
      "Installing collected packages: tensorflow, tf-keras\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.15.0\n",
      "    Uninstalling tensorflow-2.15.0:\n",
      "      Successfully uninstalled tensorflow-2.15.0\n",
      "Successfully installed tensorflow-2.16.2 tf-keras-2.16.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.3.2 -> 24.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install tf-keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0262021b",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./session21-22/classroom/6.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     11\u001b[0m threshold \u001b[38;5;241m=\u001b[39m vr\u001b[38;5;241m.\u001b[39mfind_threshold(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFacenet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m faces \u001b[38;5;241m=\u001b[39m \u001b[43mRetinaFace\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_faces\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malign\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m present_students \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# resp = RetinaFace.detect_faces(path)\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# img = cv2.imread(path)\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# plt.imshow(RGB_img)\u001b[39;00m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;66;03m# plt.show()\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\retinaface\\RetinaFace.py:280\u001b[0m, in \u001b[0;36mextract_faces\u001b[1;34m(img_path, threshold, model, align, allow_upscaling, expand_face_area)\u001b[0m\n\u001b[0;32m    275\u001b[0m nose \u001b[38;5;241m=\u001b[39m landmarks[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnose\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    276\u001b[0m \u001b[38;5;66;03m# mouth_right = landmarks[\"mouth_right\"]\u001b[39;00m\n\u001b[0;32m    277\u001b[0m \u001b[38;5;66;03m# mouth_left = landmarks[\"mouth_left\"]\u001b[39;00m\n\u001b[0;32m    278\u001b[0m \n\u001b[0;32m    279\u001b[0m \u001b[38;5;66;03m# notice that left eye of one is seen on the right from your perspective\u001b[39;00m\n\u001b[1;32m--> 280\u001b[0m aligned_img, rotate_angle, rotate_direction \u001b[38;5;241m=\u001b[39m \u001b[43mpostprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malignment_procedure\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    281\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_eye\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_eye\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_eye\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_eye\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnose\u001b[49m\n\u001b[0;32m    282\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;66;03m# find new facial area coordinates after alignment\u001b[39;00m\n\u001b[0;32m    285\u001b[0m rotated_x1, rotated_y1, rotated_x2, rotated_y2 \u001b[38;5;241m=\u001b[39m postprocess\u001b[38;5;241m.\u001b[39mrotate_facial_area(\n\u001b[0;32m    286\u001b[0m     (x, y, x \u001b[38;5;241m+\u001b[39m w, y \u001b[38;5;241m+\u001b[39m h), rotate_angle, rotate_direction, (img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    287\u001b[0m )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\retinaface\\commons\\postprocess.py:89\u001b[0m, in \u001b[0;36malignment_procedure\u001b[1;34m(img, left_eye, right_eye, nose)\u001b[0m\n\u001b[0;32m     86\u001b[0m         angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m90\u001b[39m \u001b[38;5;241m-\u001b[39m angle\n\u001b[0;32m     88\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m---> 89\u001b[0m     img \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirection\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mangle\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     91\u001b[0m     angle \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m  \u001b[38;5;66;03m# Dummy value for undefined angle\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:681\u001b[0m, in \u001b[0;36mImage.__array_interface__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    679\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtobytes(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mL\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    680\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 681\u001b[0m         new[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    682\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    683\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, (\u001b[38;5;167;01mMemoryError\u001b[39;00m, \u001b[38;5;167;01mRecursionError\u001b[39;00m)):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\PIL\\Image.py:761\u001b[0m, in \u001b[0;36mImage.tobytes\u001b[1;34m(self, encoder_name, *args)\u001b[0m\n\u001b[0;32m    758\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder error \u001b[39m\u001b[38;5;132;01m{\u001b[39;00merrcode\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m in tobytes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    759\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg)\n\u001b[1;32m--> 761\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43mb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from retinaface import RetinaFace\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from deepface.modules import verification as vr\n",
    "\n",
    "start_total = time.time()\n",
    "path = \"./session21-22/classroom/6.jpg\"\n",
    "threshold = vr.find_threshold('Facenet', 'euclidean')\n",
    "faces = RetinaFace.extract_faces(img_path = path, align = True)\n",
    "present_students = []\n",
    "\n",
    "# resp = RetinaFace.detect_faces(path)\n",
    "# img = cv2.imread(path)\n",
    "# RGB_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "# for key in resp.keys():\n",
    "#     face = resp[key]\n",
    "#     xmin = face['facial_area'][0]\n",
    "#     ymin = face['facial_area'][1]\n",
    "#     xmax = face['facial_area'][2]\n",
    "#     ymax = face['facial_area'][3]\n",
    "#     cv2.rectangle(RGB_img, (xmin, ymin), (xmax, ymax), color=(255,0,0), thickness=4)\n",
    "# plt.imshow(RGB_img)\n",
    "# plt.show()\n",
    "\n",
    "for face in faces:\n",
    "    img = Image.fromarray(face)\n",
    "    img.save('temp.jpg')\n",
    "    \n",
    "    start = time.time()\n",
    "    embedding = np.array(DeepFace.represent('./temp.jpg', model_name = 'Facenet', align=True, detector_backend='retinaface', enforce_detection = False))\n",
    "    for student in database:\n",
    "        source = student['face_embedding'][0]['embedding']\n",
    "        target = embedding[0]['embedding']\n",
    "        distance = vr.find_euclidean_distance(source, target)\n",
    "        if distance <= threshold:\n",
    "            end = time.time()\n",
    "            present_students.append(student)\n",
    "            plt.imshow(face)\n",
    "            plt.show()\n",
    "            print('Recognised Student: {recognised_student} ({ID})'.format(recognised_student = student['name'], ID=student['ID']))\n",
    "            print('Euclidean distance: {dst}'.format(dst=distance))\n",
    "            print('Time taken in embedding and recognition: {time}'.format(time = end-start))\n",
    "            break\n",
    "    \n",
    "\n",
    "end_total = time.time()\n",
    "print(\"Total time: {time}\".format(time=end_total-start_total))\n",
    "print('Number of detected faces: {detected_faces}'.format(detected_faces=len(faces)))\n",
    "print('Number of recognized students: {recognized_num}'.format(recognized_num=len(present_students)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b653f044",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install imutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489b7a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from retinaface import RetinaFace\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from deepface.modules import verification as vr\n",
    "\n",
    "def beginRecognition():\n",
    "    start_total = time.time()\n",
    "    threshold = vr.find_threshold('Facenet', 'euclidean')\n",
    "    faces = RetinaFace.extract_faces(img_path = \"./snapshot.png\", align = True)\n",
    "    present_students = []\n",
    "\n",
    "    for face in faces:\n",
    "        img = Image.fromarray(face)\n",
    "        img.save('temp.jpg')\n",
    "\n",
    "        start = time.time()\n",
    "        embedding = np.array(DeepFace.represent('./temp.jpg', model_name = 'Facenet', align=True, detector_backend='retinaface', enforce_detection = False))\n",
    "        for student in database:\n",
    "            source = student['face_embedding'][0]['embedding']\n",
    "            target = embedding[0]['embedding']\n",
    "            distance = vr.find_euclidean_distance(source, target)\n",
    "            if distance <= threshold:\n",
    "                end = time.time()\n",
    "                present_students.append(student)\n",
    "                plt.imshow(face)\n",
    "                plt.show()\n",
    "                print('Recognised Student: {recognised_student}'.format(recognised_student = student['name']))\n",
    "                print('Euclidean distance: {dst}'.format(dst=distance))\n",
    "                print('Time taken in embedding and recognition: {time}'.format(time = end-start))\n",
    "                break\n",
    "\n",
    "\n",
    "    end_total = time.time()\n",
    "    print(\"Total time: {time}\".format(time=end_total-start_total))\n",
    "    print('Number of detected faces: {detected_faces}'.format(detected_faces=len(faces)))\n",
    "    print('Number of recognized students: {recognized_num}'.format(recognized_num=len(present_students)))\n",
    "\n",
    "\n",
    "url = \"https://192.168.0.100:8080/video\"\n",
    "cap = cv2.VideoCapture(url)\n",
    "captured = False\n",
    "\n",
    "cv2.namedWindow(\"Stream\", cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow('Stream', 900, 900) \n",
    "while(True):\n",
    "    camera, frame = cap.read()\n",
    "    if frame is not None:\n",
    "        cv2.imshow(\"Stream\", frame)\n",
    "    q = cv2.waitKey(1)\n",
    "    if q==ord(\"q\"):\n",
    "        break\n",
    "    elif q%256 == 32:\n",
    "        # SPACE pressed\n",
    "        img_name = \"snapshot.png\"\n",
    "        cv2.imwrite(img_name, frame)\n",
    "        print(\"{} written!\".format(img_name))\n",
    "        captured = True\n",
    "        break\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "if captured:\n",
    "    beginRecognition()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1c05288",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Student Name:Sultanul Arefin\n",
      "Student ID:22702037\n",
      "Data entered successfully!\n",
      "Student Name:Ayead Mujib Tamim\n",
      "Student ID:22702035\n",
      "Data entered successfully!\n",
      "Student Name:Kollol Sarkar\n",
      "Student ID:22702017\n",
      "Data entered successfully!\n",
      "Student Name:Abdullah Al Shakib\n",
      "Student ID:22702059\n",
      "Data entered successfully!\n",
      "Student Name:Atikur Rahman\n",
      "Student ID:22702049\n",
      "Data entered successfully!\n",
      "Student Name:Isabah Sharar\n",
      "Student ID:22702057\n",
      "Data entered successfully!\n",
      "Student Name:Dibakor Roy\n",
      "Student ID:22702054\n",
      "Data entered successfully!\n",
      "Student Name:Akram Hossain Ifty\n",
      "Student ID:22702013\n",
      "Data entered successfully!\n",
      "Student Name:Afia Farzana\n",
      "Student ID:22702018\n",
      "Data entered successfully!\n",
      "Student Name:Tasnim Tabassum Anika\n",
      "Student ID:22702001\n",
      "Data entered successfully!\n",
      "Student Name:Tayaba Haque Chaity\n",
      "Student ID:22702012\n",
      "Data entered successfully!\n",
      "Student Name:Zarifa Raiyan\n",
      "Student ID:21702068\n",
      "Data entered successfully!\n",
      "Student Name:Mohammudul Islam Raihan\n",
      "Student ID:22702005\n",
      "Data entered successfully!\n",
      "Student Name:Sheikh Abdul Mobin\n",
      "Student ID:22702020\n",
      "Data entered successfully!\n",
      "Student Name:Shaleh Zaed\n",
      "Student ID:22702063\n",
      "Data entered successfully!\n",
      "Student Name:Rased Khan\n",
      "Student ID:22702031\n",
      "Data entered successfully!\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "from retinaface import RetinaFace\n",
    "import matplotlib.pyplot as plt\n",
    "from deepface import DeepFace\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import time\n",
    "from deepface.modules import verification as vr\n",
    "import json\n",
    "\n",
    "\n",
    "def inputStudentInformation():\n",
    "    name = input(\"Student Name:\")\n",
    "    ID = input(\"Student ID:\")\n",
    "\n",
    "    url = \"https://192.168.7.133:8080/video\"\n",
    "    cap = cv2.VideoCapture(url)\n",
    "    captured = False\n",
    "    cv2.namedWindow(\"Stream\", cv2.WINDOW_NORMAL)\n",
    "    cv2.resizeWindow('Stream', 900, 900) \n",
    "\n",
    "    while(True):\n",
    "        camera, frame = cap.read()\n",
    "        if frame is not None:\n",
    "            dispFrame = cv2.resize(frame, (200, 200))\n",
    "            cv2.imshow(\"Stream\", dispFrame)\n",
    "        q = cv2.waitKey(1)\n",
    "        if q==ord(\"q\"):\n",
    "            break\n",
    "        elif q%256 == 32:\n",
    "            # SPACE pressed\n",
    "            img_name = \"./session21-22/{filename}.png\".format(filename=ID)\n",
    "            cv2.imwrite(img_name, frame)\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n",
    "    temp = {\n",
    "        'name': name,\n",
    "        'ID': ID,\n",
    "        'image': \"./session21-22/{filename}.png\".format(filename=ID)\n",
    "    }\n",
    "\n",
    "    f = open('./session21-22/21-22.json')\n",
    "    training_data = json.load(f)\n",
    "    training_data.append(temp)\n",
    "    f.close()\n",
    "\n",
    "    with open('./session21-22/21-22.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(training_data, f, ensure_ascii=False, indent=4)\n",
    "    f.close()\n",
    "    print(\"Data entered successfully!\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        inputStudentInformation()\n",
    "except KeyboardInterrupt:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41c3125",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
